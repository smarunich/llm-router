{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task Router "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will explore NVIDIA's prompt classification model that analyzes prompts based on task type. The model serves as an intelligent router for directing prompts to appropriate processing pipelines.\n",
    "\n",
    "## Model Information\n",
    "* Source: [NVIDIA's prompt-task-and-complexity-classifier](https://huggingface.co/nvidia/prompt-task-and-complexity-classifier)\n",
    "* Architecture: DeBERTa-v3-base backbone\n",
    "* Purpose: Multi-task classification for prompt analysis\n",
    "* Output: Task types and multiple complexity metrics\n",
    "\n",
    "## Task Categories\n",
    "The model classifies prompts into various task types\n",
    "\n",
    "![task_types](assets/Task_Categorization.png \"Task Type Categories\")\n",
    "\n",
    "* Open QA (Open Question-Answering)\n",
    "    * Questions requiring general world knowledge without specific context\n",
    "    * Example: \"What causes earthquakes and how do they occur?\"\n",
    "* Closed QA (Closed Question-Answering)\n",
    "    * Questions requiring analysis of provided information/context\n",
    "    * Example: \"Based on the patient's symptoms described above, what is the likely diagnosis?\"\n",
    "* Summarization\n",
    "    * Tasks requiring condensing longer text into key points\n",
    "    * Example: \"Summarize this research paper about climate change in three sentences.\"\n",
    "* Text Generation\n",
    "    * Creating original content based on given parameters\n",
    "    * Example: \"Write a product description for a new smartphone.\"\n",
    "* Code Generation\n",
    "    * Creating or completing programming code\n",
    "    * Example: \"Write a Python script that uses a for loop to calculate Fibonacci numbers.\"\n",
    "* Chatbot\n",
    "    * Conversational interactions requiring context maintenance\n",
    "    * Example: \"I need help tracking my order status.\"\n",
    "* Classification\n",
    "    * Categorizing or labeling content into predefined groups\n",
    "    * Example: \"Is this customer review positive or negative?\"\n",
    "* Rewrite\n",
    "    * Rephrasing existing content while maintaining meaning\n",
    "    * Example: \"Rewrite this paragraph in simpler terms for a middle school student.\"\n",
    "* Brainstorming\n",
    "    * Creative ideation and generation of multiple options\n",
    "    * Example: \"Generate five unique marketing campaign ideas for a new coffee shop.\"\n",
    "* Extraction\n",
    "    * Pulling specific information from provided content\n",
    "    * Example: \"Extract all the dates and locations mentioned in this news article.\"\n",
    "* Other\n",
    "    * Tasks that don't fit into standard categories\n",
    "    * Example: \"Help me debug why my printer isn't working.\"\n",
    "\n",
    "## Routing Suggestions\n",
    "\n",
    "Since some of the tasks overlap and have very negligible differences in their probability distribution, we can group some tasks into the same LLM endpoint\n",
    "\n",
    "* Code\n",
    "* Open-QA\n",
    "* (Closed QA, Extraction)\n",
    "* (Rewrite, Summarization, Text Generation)\n",
    "* Classification\n",
    "* ChatBot\n",
    "* (Other, Unknown)\n",
    "* Brainstorming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from huggingface_hub import PyTorchModelHubMixin\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "\n",
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "\n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = (\n",
    "            attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        )\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings\n",
    "\n",
    "\n",
    "class MulticlassHead(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(MulticlassHead, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class CustomModel(nn.Module, PyTorchModelHubMixin):\n",
    "    def __init__(self, target_sizes, task_type_map, weights_map, divisor_map):\n",
    "        super(CustomModel, self).__init__()\n",
    "\n",
    "        self.backbone = AutoModel.from_pretrained(\"microsoft/DeBERTa-v3-base\")\n",
    "        self.target_sizes = target_sizes.values()\n",
    "        self.task_type_map = task_type_map\n",
    "        self.weights_map = weights_map\n",
    "        self.divisor_map = divisor_map\n",
    "\n",
    "        self.heads = [\n",
    "            MulticlassHead(self.backbone.config.hidden_size, sz)\n",
    "            for sz in self.target_sizes\n",
    "        ]\n",
    "\n",
    "        for i, head in enumerate(self.heads):\n",
    "            self.add_module(f\"head_{i}\", head)\n",
    "\n",
    "        self.pool = MeanPooling()\n",
    "\n",
    "    def forward(self, batch):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        mean_pooled_representation = self.pool(last_hidden_state, attention_mask)\n",
    "\n",
    "        logits = [\n",
    "            self.heads[k](mean_pooled_representation)\n",
    "            for k in range(len(self.target_sizes))\n",
    "        ]\n",
    "\n",
    "        # return self.process_logits(logits)\n",
    "        return logits\n",
    "\n",
    "class LogitsProcessor:\n",
    "    def __init__(self, task_type_map, weights_map, divisor_map):\n",
    "        self.task_type_map = task_type_map\n",
    "        self.weights_map = weights_map\n",
    "        self.divisor_map = divisor_map\n",
    "        # self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.targets = [\n",
    "            \"task_type\", \"creativity_scope\", \"reasoning\", \"contextual_knowledge\",\n",
    "            \"number_of_few_shots\", \"domain_knowledge\", \"no_label_reason\", \"constraint_ct\"\n",
    "        ]\n",
    "\n",
    "    def compute_results(self, preds, target, decimal=4):\n",
    "        if target == \"task_type\":\n",
    "            task_type = {}\n",
    "\n",
    "            top2_indices = torch.topk(preds, k=2, dim=1).indices\n",
    "            softmax_probs = torch.softmax(preds, dim=1)\n",
    "            top2_probs = softmax_probs.gather(1, top2_indices)\n",
    "            top2 = top2_indices.detach().cpu().tolist()\n",
    "            top2_prob = top2_probs.detach().cpu().tolist()\n",
    "\n",
    "            top2_strings = [\n",
    "                [self.task_type_map[str(idx)] for idx in sample] for sample in top2\n",
    "            ]\n",
    "            top2_prob_rounded = [\n",
    "                [round(value, 3) for value in sublist] for sublist in top2_prob\n",
    "            ]\n",
    "\n",
    "            counter = 0\n",
    "            for sublist in top2_prob_rounded:\n",
    "                if sublist[1] < 0.1:\n",
    "                    top2_strings[counter][1] = \"NA\"\n",
    "                counter += 1\n",
    "\n",
    "            task_type_1 = [sublist[0] for sublist in top2_strings]\n",
    "            task_type_2 = [sublist[1] for sublist in top2_strings]\n",
    "            task_type_prob = [sublist[0] for sublist in top2_prob_rounded]\n",
    "\n",
    "            return (task_type_1, task_type_2, task_type_prob)\n",
    "        else:\n",
    "            preds = torch.softmax(preds, dim=1)\n",
    "\n",
    "            weights = np.array(self.weights_map[target])\n",
    "            weighted_sum = np.sum(np.array(preds.detach().cpu()) * weights, axis=1)\n",
    "            scores = weighted_sum / self.divisor_map[target]\n",
    "\n",
    "            scores = [round(value, decimal) for value in scores]\n",
    "            if target == \"number_of_few_shots\":\n",
    "                scores = [x if x >= 0.05 else 0 for x in scores]\n",
    "            return scores\n",
    "\n",
    "    def process_logits(self, logits):\n",
    "        result = {}\n",
    "\n",
    "        for i, target in enumerate(self.targets):\n",
    "            logits_tensor = torch.from_numpy(logits[i]).float()\n",
    "            \n",
    "            if target == \"task_type\":\n",
    "                task_type_results = self.compute_results(logits_tensor, target=target)\n",
    "                result[\"task_type_1\"] = task_type_results[0]\n",
    "                result[\"task_type_2\"] = task_type_results[1]\n",
    "                result[\"task_type_prob\"] = task_type_results[2]\n",
    "            else:\n",
    "                result[target] = self.compute_results(logits_tensor, target=target)\n",
    "\n",
    "        # Calculate prompt_complexity_score\n",
    "        result[\"prompt_complexity_score\"] = [\n",
    "            round(\n",
    "                0.35 * creativity\n",
    "                + 0.25 * reasoning\n",
    "                + 0.15 * constraint\n",
    "                + 0.15 * domain_knowledge\n",
    "                + 0.05 * contextual_knowledge\n",
    "                + 0.05 * few_shots,\n",
    "                5,\n",
    "            )\n",
    "            for creativity, reasoning, constraint, domain_knowledge, contextual_knowledge, few_shots in zip(\n",
    "                result[\"creativity_scope\"],\n",
    "                result[\"reasoning\"],\n",
    "                result[\"constraint_ct\"],\n",
    "                result[\"domain_knowledge\"],\n",
    "                result[\"contextual_knowledge\"],\n",
    "                result[\"number_of_few_shots\"],\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\"nvidia/prompt-task-and-complexity-classifier\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"nvidia/prompt-task-and-complexity-classifier\"\n",
    ")\n",
    "model = CustomModel(\n",
    "    target_sizes=config.target_sizes,\n",
    "    task_type_map=config.task_type_map,\n",
    "    weights_map=config.weights_map,\n",
    "    divisor_map=config.divisor_map,\n",
    ").from_pretrained(\"nvidia/prompt-task-and-complexity-classifier\")\n",
    "model.eval()\n",
    "\n",
    "prompt = [\"Prompt: Write a Python script that uses a for loop.\"]\n",
    "\n",
    "encoded_texts = tokenizer(\n",
    "    prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    add_special_tokens=True,\n",
    "    max_length=512,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    ")\n",
    "\n",
    "result = model(encoded_texts)\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Router Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see the model returns a list of logits from multiple classification heads, which isn't ideal for\n",
    "Triton inference server deployment, In order to address the need to handle multiple output tensors from NVIDIA's prompt classifier model we need to creating a wrapper that concatenates the outputs into a single tensor.\n",
    "\n",
    "### WrapperModel Class\n",
    "* Takes the original model as input\n",
    "* Concatenates multiple output tensors into a single tensor\n",
    "* Simplifies the output format for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\"nvidia/prompt-task-and-complexity-classifier\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nvidia/prompt-task-and-complexity-classifier\")\n",
    "\n",
    "class TracedModel(nn.Module, PyTorchModelHubMixin):\n",
    "    def __init__(self, target_sizes, task_type_map, weights_map, divisor_map):\n",
    "        super(TracedModel, self).__init__()\n",
    "        self.backbone = AutoModel.from_pretrained(\"microsoft/DeBERTa-v3-base\")\n",
    "        self.target_sizes = target_sizes.values()\n",
    "        self.task_type_map = task_type_map\n",
    "        self.weights_map = weights_map\n",
    "        self.divisor_map = divisor_map\n",
    "\n",
    "        self.heads = [\n",
    "            MulticlassHead(self.backbone.config.hidden_size, sz)\n",
    "            for sz in self.target_sizes\n",
    "        ]\n",
    "\n",
    "        for i, head in enumerate(self.heads):\n",
    "            self.add_module(f\"head_{i}\", head)\n",
    "\n",
    "        self.pool = MeanPooling()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        mean_pooled_representation = self.pool(last_hidden_state, attention_mask)\n",
    "\n",
    "        logits = [\n",
    "            self.heads[k](mean_pooled_representation)\n",
    "            for k in range(len(self.target_sizes))\n",
    "        ]\n",
    "        return logits\n",
    "\n",
    "class WrapperModel(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super().__init__()\n",
    "        self.model = original_model\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids, attention_mask)\n",
    "        return torch.cat(outputs, dim=1)\n",
    "\n",
    "model = TracedModel(\n",
    "    target_sizes=config.target_sizes,\n",
    "    task_type_map=config.task_type_map,\n",
    "    weights_map=config.weights_map,\n",
    "    divisor_map=config.divisor_map,\n",
    ").from_pretrained(\"nvidia/prompt-task-and-complexity-classifier\")\n",
    "\n",
    "model = model.to('cuda')\n",
    "model.eval()\n",
    "\n",
    "wrapped_model = WrapperModel(model)\n",
    "wrapped_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write a Python script that uses a for loop.\"\n",
    "encoded_texts = tokenizer(\n",
    "    [prompt],\n",
    "    return_tensors=\"pt\",\n",
    "    add_special_tokens=True,\n",
    "    max_length=512,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trace the wrapped model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    wrapped_model = torch.jit.trace(\n",
    "        wrapped_model,\n",
    "        (\n",
    "            encoded_texts[\"input_ids\"].to('cuda'),\n",
    "            encoded_texts[\"attention_mask\"].to('cuda')\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_model.save(\"triton_template/task_router/1/model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the saved model and compare it with original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nvidia/prompt-task-and-complexity-classifier\")\n",
    "\n",
    "# Load the traced model\n",
    "wrapped_model = torch.jit.load('prompt_classifier_traced.pt')\n",
    "wrapped_model.eval()\n",
    "\n",
    "# Prepare a sample input\n",
    "sample_text = \"Prompt: Translate the following sentence from English to French: 'Hello, how are you?'\"\n",
    "inputs = tokenizer(sample_text, return_tensors=\"pt\", max_length=512, padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Move inputs to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_ids = inputs['input_ids'].to(device)\n",
    "attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    output = wrapped_model(input_ids, attention_mask)\n",
    "\n",
    "# Process the output\n",
    "def process_results(output_tensor, target_sizes):\n",
    "    results = []\n",
    "    start_idx = 0\n",
    "    for size in target_sizes:\n",
    "        end_idx = start_idx + size\n",
    "        result = output_tensor[:, start_idx:end_idx]\n",
    "        results.append(result)\n",
    "        start_idx = end_idx\n",
    "    return results\n",
    "\n",
    "processed_results = process_results(output, config.target_sizes.values())\n",
    "\n",
    "# Interpret the results\n",
    "task_names = list(config.target_sizes.keys())\n",
    "for i, result in enumerate(processed_results):\n",
    "    probabilities = torch.softmax(result, dim=1)\n",
    "    predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "    confidence = probabilities[0, predicted_class].item()\n",
    "    print(f\"{task_names[i]}: Predicted class = {predicted_class}, Confidence = {confidence:.4f}\")\n",
    "\n",
    "# Compare with the original model (optional)\n",
    "original_model = TracedModel(\n",
    "    target_sizes=config.target_sizes,\n",
    "    task_type_map=config.task_type_map,\n",
    "    weights_map=config.weights_map,\n",
    "    divisor_map=config.divisor_map,\n",
    ").from_pretrained(\"nvidia/prompt-task-and-complexity-classifier\")\n",
    "original_model = original_model.to(device)\n",
    "original_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    original_outputs = original_model(input_ids, attention_mask)\n",
    "    wrapped_original_output = torch.cat(original_outputs, dim=1)\n",
    "\n",
    "print(\"\\nComparing outputs:\")\n",
    "print(\"Original model output shape:\", wrapped_original_output.shape)\n",
    "print(\"Traced model output shape:\", output.shape)\n",
    "print(\"Outputs match:\", torch.allclose(wrapped_original_output, output, atol=1e-4))\n",
    "\n",
    "separated_original_outputs = process_results(wrapped_original_output, config.target_sizes.values())\n",
    "\n",
    "# Compare separated outputs\n",
    "print(\"\\nComparing separated outputs:\")\n",
    "for i, (original, traced) in enumerate(zip(separated_original_outputs, processed_results)):\n",
    "    print(f\"Task {i}:\")\n",
    "    print(f\"  Original output shape: {original.shape}\")\n",
    "    print(f\"  Traced output shape: {traced.shape}\")\n",
    "    print(f\"  Outputs match: {torch.allclose(original, traced, atol=1e-4)}\")\n",
    "\n",
    "# Interpret the original model results\n",
    "print(\"\\nOriginal model results:\")\n",
    "for i, result in enumerate(separated_original_outputs):\n",
    "    probabilities = torch.softmax(result, dim=1)\n",
    "    predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "    confidence = probabilities[0, predicted_class].item()\n",
    "    print(f\"{task_names[i]}: Predicted class = {predicted_class}, Confidence = {confidence:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_original_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triton Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the traced model in the torch script, we can add this to the [Triton Inference Server](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/contents.html) and use the [ensemble pipeline feature](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/ensemble_models.html) to set up the pre and post-processing pipeline. \n",
    "\n",
    "The pre and post-processing code is available under the `triton_template/preprocessing_task_router/` and `triton_template/postprocessing_task_router/` directories and the `triton_template/task_router_ensemble/` contains the config on how the pre-processing, model and post-processing are linked together. \n",
    "\n",
    "This will be the same as the code downloaded from NGC when setting up the default task router.\n",
    "\n",
    "This is organized in the following structure in the `/routers` directory with the following format\n",
    "\n",
    "```\n",
    "model_repository/\n",
    "├── task_router\n",
    "│   ├── 1\n",
    "│   │   └── model.pt\n",
    "│   └── config.pbtxt\n",
    "├── task_router_ensemble\n",
    "│   ├── 1\n",
    "│   └── config.pbtxt\n",
    "├── postprocessing_task_router\n",
    "│   ├── 1\n",
    "│   │   ├── logits_processor.py\n",
    "│   │   ├── model.py\n",
    "│   │   └── __pycache__\n",
    "│   │       ├── logits_processor.cpython-310.pyc\n",
    "│   │       └── model.cpython-310.pyc\n",
    "│   └── config.pbtxt\n",
    "└── preprocessing_task_router\n",
    "    ├── 1\n",
    "    │   ├── model.py\n",
    "    │   └── __pycache__\n",
    "    │       └── model.cpython-310.pyc\n",
    "    └── config.pbtxt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now copy the contents of `triton_template/` folder to the `/model_repository` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r triton_template/* /model_repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On your original machine, not within the Docker JupyterLab notebook, start the router server by running `make up`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v http://router-server:8000/v2/models/task_router_ensemble/ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tritonclient.http as httpclient\n",
    "from transformers import AutoConfig\n",
    "\n",
    "def send_request(triton_client, text):\n",
    "    input_text = np.array([[text]], dtype=object)\n",
    "    inputs = [httpclient.InferInput(\"INPUT\", input_text.shape, \"BYTES\")]\n",
    "    inputs[0].set_data_from_numpy(input_text)\n",
    "\n",
    "    outputs = [httpclient.InferRequestedOutput(\"OUTPUT\")]\n",
    "\n",
    "    response = triton_client.infer(model_name=\"task_router_ensemble\", inputs=inputs, outputs=outputs)\n",
    "    return response\n",
    "\n",
    "# Load the config\n",
    "config = AutoConfig.from_pretrained(\"nvidia/prompt-task-and-complexity-classifier\")\n",
    "\n",
    "# Get the task types from the config\n",
    "task_types = list(config.task_type_map.values())\n",
    "\n",
    "triton_client = httpclient.InferenceServerClient(url=\"router-server:8000\")\n",
    "\n",
    "prompt = \"Prompt: Antibiotics are a type of medication used to treat bacterial infections. They work by either killing the bacteria or preventing them from reproducing, allowing the body’s immune system to fight off the infection. Antibiotics are usually taken orally in the form of pills, capsules, or liquid solutions, or sometimes administered intravenously. They are not effective against viral infections, and using them inappropriately can lead to antibiotic resistance. Explain the above in one sentence.\"\n",
    "result = send_request(triton_client, prompt)\n",
    "\n",
    "output_data = result.as_numpy(\"OUTPUT\")\n",
    "\n",
    "# Find the index of the maximum value (which should be 1 in the one-hot vector)\n",
    "predicted_task_index = np.argmax(output_data)\n",
    "\n",
    "# Map the index to the corresponding task type\n",
    "predicted_task = task_types[predicted_task_index]\n",
    "\n",
    "print(f\"Input prompt: {prompt}\")\n",
    "print(f\"Predicted task type: {predicted_task}\")\n",
    "print(f\"One-hot encoded output: {output_data}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
